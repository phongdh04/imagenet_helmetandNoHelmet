# =============================================================================
# HELMET_DETECTION_MODEL.PY - M√î H√åNH PH√ÅT HI·ªÜN M≈® B·∫¢O HI·ªÇM
# =============================================================================
# M·ª•c ƒë√≠ch: X√¢y d·ª±ng v√† hu·∫•n luy·ªán m√¥ h√¨nh CNN ƒë·ªÉ ph√¢n lo·∫°i c√≥/kh√¥ng c√≥ m≈© b·∫£o hi·ªÉm
# Ki·∫øn tr√∫c: MobileNetV2 v·ªõi Transfer Learning v√† Fine-tuning
# Input: Dataset ƒë√£ ƒë∆∞·ª£c ti·ªÅn x·ª≠ l√Ω (128x128 pixels)
# Output: M√¥ h√¨nh ƒë√£ hu·∫•n luy·ªán v√† ƒë√°nh gi√° hi·ªáu su·∫•t
# T√°c gi·∫£: D·ª± √°n ph√°t hi·ªán m≈© b·∫£o hi·ªÉm
# =============================================================================

import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications import MobileNetV2
from tensorflow.keras.layers import GlobalAveragePooling2D, Dropout, Dense
from tensorflow.keras.regularizers import l2
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
import matplotlib.pyplot as plt
import numpy as np
import os

class HelmetDetectionModel:
    """
    L·ªõp m√¥ h√¨nh ph√°t hi·ªán m≈© b·∫£o hi·ªÉm s·ª≠ d·ª•ng MobileNetV2
    """
    
    def __init__(self, input_shape=(128, 128, 3), num_classes=1):
        """
        Kh·ªüi t·∫°o m√¥ h√¨nh
        
        Args:
            input_shape (tuple): K√≠ch th∆∞·ªõc input (height, width, channels)
            num_classes (int): S·ªë l∆∞·ª£ng class (1 cho binary classification)
        """
        self.input_shape = input_shape
        self.num_classes = num_classes
        self.model = None
        self.history = None
        self.fine_tune_history = None
        
    def load_data(self, data_dir, validation_split=0.2, batch_size=32):
        """
        T·∫£i v√† chu·∫©n b·ªã d·ªØ li·ªáu cho hu·∫•n luy·ªán
        
        Args:
            data_dir (str): ƒê∆∞·ªùng d·∫´n ƒë·∫øn th∆∞ m·ª•c dataset
            validation_split (float): T·ª∑ l·ªá validation (0.2 = 20%)
            batch_size (int): K√≠ch th∆∞·ªõc batch
        """
        print("üîÑ ƒêang t·∫£i d·ªØ li·ªáu...")
        
        # C·∫•u h√¨nh ImageDataGenerator cho training v·ªõi augmentation
        train_datagen = ImageDataGenerator(
            rescale=1./255,              # Chu·∫©n h√≥a pixel values v·ªÅ [0,1]
            validation_split=validation_split,
            fill_mode='nearest',         # C√°ch l·∫•p ƒë·∫ßy pixel m·ªõi
            width_shift_range=0.1,      # D·ªãch chuy·ªÉn ngang ¬±10%
            height_shift_range=0.1,     # D·ªãch chuy·ªÉn d·ªçc ¬±10%
            rotation_range=30,           # Xoay ·∫£nh ¬±30 ƒë·ªô
            zoom_range=0.2,              # Thu ph√≥ng ¬±20%
            shear_range=0.1,             # C·∫Øt x√©n ¬±10%
            horizontal_flip=True         # L·∫≠t ngang
        )
        
        # C·∫•u h√¨nh ImageDataGenerator cho validation (ch·ªâ rescale)
        val_datagen = ImageDataGenerator(
            rescale=1./255,
            validation_split=validation_split
        )
        
        # T·∫°o training dataset
        self.train_ds = train_datagen.flow_from_directory(
            data_dir,
            target_size=(self.input_shape[0], self.input_shape[1]),
            batch_size=batch_size,
            class_mode='binary',        # Binary classification
            subset='training',
            seed=123
        )
        
        # T·∫°o validation dataset
        self.val_ds = val_datagen.flow_from_directory(
            data_dir,
            target_size=(self.input_shape[0], self.input_shape[1]),
            batch_size=batch_size,
            class_mode='binary',
            subset='validation',
            seed=123
        )
        
        print(f"‚úÖ Training samples: {self.train_ds.samples}")
        print(f"‚úÖ Validation samples: {self.val_ds.samples}")
        print(f"‚úÖ Classes: {self.train_ds.class_indices}")
        
    def build_model(self, dropout_rate=0.5, l2_reg=0.001):
        """
        X√¢y d·ª±ng m√¥ h√¨nh MobileNetV2 v·ªõi transfer learning
        
        Args:
            dropout_rate (float): T·ª∑ l·ªá dropout ƒë·ªÉ ngƒÉn overfitting
            l2_reg (float): H·ªá s·ªë L2 regularization
        """
        print("üîÑ ƒêang x√¢y d·ª±ng m√¥ h√¨nh...")
        
        # 1. T·∫£i MobileNetV2 pre-trained tr√™n ImageNet
        base_model = MobileNetV2(
            input_shape=self.input_shape,
            include_top=False,           # Lo·∫°i b·ªè top classification layer
            weights='imagenet'           # S·ª≠ d·ª•ng weights ƒë√£ train tr√™n ImageNet
        )
        
        # 2. ƒê√≥ng bƒÉng to√†n b·ªô base model (freeze weights)
        for layer in base_model.layers:
            layer.trainable = False
        
        print(f"‚úÖ Base model: MobileNetV2 v·ªõi {len(base_model.layers)} layers")
        print(f"‚úÖ Base model ƒë√£ ƒë∆∞·ª£c ƒë√≥ng bƒÉng")
        
        # 3. X√¢y d·ª±ng top layers cho binary classification
        x = GlobalAveragePooling2D()(base_model.output)  # Global average pooling
        x = Dense(128, activation='relu', 
                 kernel_regularizer=l2(l2_reg))(x)       # Dense layer v·ªõi L2 reg
        x = Dropout(dropout_rate)(x)                      # Dropout ƒë·ªÉ ngƒÉn overfitting
        predictions = Dense(self.num_classes, 
                           activation='sigmoid',          # Sigmoid cho binary classification
                           kernel_regularizer=l2(l2_reg))(x)
        
        # 4. T·∫°o model ho√†n ch·ªânh
        self.model = Model(inputs=base_model.input, outputs=predictions)
        
        print("‚úÖ M√¥ h√¨nh ƒë√£ ƒë∆∞·ª£c x√¢y d·ª±ng th√†nh c√¥ng!")
        self.model.summary()
        
    def compile_model(self, learning_rate=1e-3):
        """
        Compile m√¥ h√¨nh v·ªõi optimizer, loss function v√† metrics
        
        Args:
            learning_rate (float): Learning rate cho optimizer
        """
        self.model.compile(
            optimizer=Adam(learning_rate=learning_rate),
            loss='binary_crossentropy',      # Binary crossentropy cho binary classification
            metrics=['accuracy']             # Accuracy metric
        )
        print(f"‚úÖ M√¥ h√¨nh ƒë√£ ƒë∆∞·ª£c compile v·ªõi learning rate: {learning_rate}")
        
    def train_top_layers(self, epochs=20, patience=10):
        """
        Hu·∫•n luy·ªán top layers (giai ƒëo·∫°n 1)
        
        Args:
            epochs (int): S·ªë epochs t·ªëi ƒëa
            patience (int): S·ªë epochs ch·ªù tr∆∞·ªõc khi early stopping
        """
        print("üîÑ B·∫Øt ƒë·∫ßu hu·∫•n luy·ªán top layers...")
        
        # Callbacks
        early_stopping = EarlyStopping(
            monitor='val_loss',
            patience=patience,
            restore_best_weights=True
        )
        
        model_checkpoint = ModelCheckpoint(
            'best_model_top_layers.keras',
            monitor='val_loss',
            save_best_only=True
        )
        
        # Hu·∫•n luy·ªán
        self.history = self.model.fit(
            self.train_ds,
            validation_data=self.val_ds,
            epochs=epochs,
            callbacks=[early_stopping, model_checkpoint]
        )
        
        print("‚úÖ Ho√†n th√†nh hu·∫•n luy·ªán top layers!")
        
    def fine_tune_model(self, unfreeze_layers=20, learning_rate=1e-5, epochs=30):
        """
        Fine-tune base model (giai ƒëo·∫°n 2)
        
        Args:
            unfreeze_layers (int): S·ªë layers cu·ªëi c√πng s·∫Ω ƒë∆∞·ª£c unfreeze
            learning_rate (float): Learning rate th·∫•p h∆°n cho fine-tuning
            epochs (int): S·ªë epochs t·ªëi ƒëa
        """
        print(f"üîÑ B·∫Øt ƒë·∫ßu fine-tuning {unfreeze_layers} layers cu·ªëi...")
        
        # Unfreeze m·ªôt s·ªë layers cu·ªëi c·ªßa base model
        base_model = self.model.layers[1]  # MobileNetV2 layer
        for layer in base_model.layers[-unfreeze_layers:]:
            layer.trainable = True
        
        print(f"‚úÖ ƒê√£ unfreeze {unfreeze_layers} layers cu·ªëi c·ªßa base model")
        
        # Re-compile v·ªõi learning rate th·∫•p h∆°n
        self.model.compile(
            optimizer=Adam(learning_rate=learning_rate),
            loss='binary_crossentropy',
            metrics=['accuracy']
        )
        
        # Callbacks
        early_stopping = EarlyStopping(
            monitor='val_loss',
            patience=10,
            restore_best_weights=True
        )
        
        model_checkpoint = ModelCheckpoint(
            'best_model_final.keras',
            monitor='val_loss',
            save_best_only=True
        )
        
        # Fine-tuning
        self.fine_tune_history = self.model.fit(
            self.train_ds,
            validation_data=self.val_ds,
            epochs=epochs,
            callbacks=[early_stopping, model_checkpoint]
        )
        
        print("‚úÖ Ho√†n th√†nh fine-tuning!")
        
    def evaluate_model(self):
        """
        ƒê√°nh gi√° m√¥ h√¨nh tr√™n validation set
        """
        print("üîÑ ƒêang ƒë√°nh gi√° m√¥ h√¨nh...")
        
        # ƒê√°nh gi√° tr√™n validation set
        loss, accuracy = self.model.evaluate(self.val_ds)
        
        print(f"‚úÖ Validation Loss: {loss:.4f}")
        print(f"‚úÖ Validation Accuracy: {accuracy:.4f}")
        
        return loss, accuracy
        
    def plot_training_history(self):
        """
        V·∫Ω bi·ªÉu ƒë·ªì qu√° tr√¨nh hu·∫•n luy·ªán
        """
        if self.history is None:
            print("‚ö†Ô∏è  Ch∆∞a c√≥ l·ªãch s·ª≠ hu·∫•n luy·ªán ƒë·ªÉ v·∫Ω")
            return
            
        # T·∫°o subplot cho training history
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        
        # Training accuracy
        axes[0, 0].plot(self.history.history['accuracy'], label='Training Accuracy')
        axes[0, 0].plot(self.history.history['val_accuracy'], label='Validation Accuracy')
        axes[0, 0].set_title('Training v√† Validation Accuracy (Top Layers)')
        axes[0, 0].set_xlabel('Epoch')
        axes[0, 0].set_ylabel('Accuracy')
        axes[0, 0].legend()
        axes[0, 0].grid(True)
        
        # Training loss
        axes[0, 1].plot(self.history.history['loss'], label='Training Loss')
        axes[0, 1].plot(self.history.history['val_loss'], label='Validation Loss')
        axes[0, 1].set_title('Training v√† Validation Loss (Top Layers)')
        axes[0, 1].set_xlabel('Epoch')
        axes[0, 1].set_ylabel('Loss')
        axes[0, 1].legend()
        axes[0, 1].grid(True)
        
        # Fine-tuning history (n·∫øu c√≥)
        if self.fine_tune_history is not None:
            # Fine-tuning accuracy
            axes[1, 0].plot(self.fine_tune_history.history['accuracy'], label='Training Accuracy')
            axes[1, 0].plot(self.fine_tune_history.history['val_accuracy'], label='Validation Accuracy')
            axes[1, 0].set_title('Training v√† Validation Accuracy (Fine-tuning)')
            axes[1, 0].set_xlabel('Epoch')
            axes[1, 0].set_ylabel('Accuracy')
            axes[1, 0].legend()
            axes[1, 0].grid(True)
            
            # Fine-tuning loss
            axes[1, 1].plot(self.fine_tune_history.history['loss'], label='Training Loss')
            axes[1, 1].plot(self.fine_tune_history.history['val_loss'], label='Validation Loss')
            axes[1, 1].set_title('Training v√† Validation Loss (Fine-tuning)')
            axes[1, 1].set_xlabel('Epoch')
            axes[1, 1].set_ylabel('Loss')
            axes[1, 1].legend()
            axes[1, 1].grid(True)
        
        plt.tight_layout()
        plt.show()
        
    def save_model(self, filepath='helmet_detection_model.keras'):
        """
        L∆∞u m√¥ h√¨nh ƒë√£ hu·∫•n luy·ªán
        
        Args:
            filepath (str): ƒê∆∞·ªùng d·∫´n file ƒë·ªÉ l∆∞u model
        """
        self.model.save(filepath)
        print(f"‚úÖ M√¥ h√¨nh ƒë√£ ƒë∆∞·ª£c l∆∞u t·∫°i: {filepath}")

def main():
    """
    H√†m ch√≠nh ƒë·ªÉ ch·∫°y to√†n b·ªô pipeline hu·∫•n luy·ªán
    """
    print("=" * 70)
    print("M√î H√åNH PH√ÅT HI·ªÜN M≈® B·∫¢O HI·ªÇM")
    print("=" * 70)
    
    # Kh·ªüi t·∫°o m√¥ h√¨nh
    model = HelmetDetectionModel(input_shape=(128, 128, 3))
    
    # C·∫•u h√¨nh ƒë∆∞·ªùng d·∫´n dataset
    # C√≥ th·ªÉ thay ƒë·ªïi t√πy theo m√¥i tr∆∞·ªùng
    data_dir_colab = '/content/drive/MyDrive/resized_dataset'
    data_dir_local = './resized_dataset'
    
    data_dir = data_dir_colab if os.path.exists(data_dir_colab) else data_dir_local
    
    print(f"üìÅ Dataset directory: {data_dir}")
    
    # B∆∞·ªõc 1: T·∫£i d·ªØ li·ªáu
    print("\n" + "=" * 50)
    print("B∆Ø·ªöC 1: T·∫¢I D·ªÆ LI·ªÜU")
    print("=" * 50)
    model.load_data(data_dir, validation_split=0.2, batch_size=32)
    
    # B∆∞·ªõc 2: X√¢y d·ª±ng m√¥ h√¨nh
    print("\n" + "=" * 50)
    print("B∆Ø·ªöC 2: X√ÇY D·ª∞NG M√î H√åNH")
    print("=" * 50)
    model.build_model(dropout_rate=0.5, l2_reg=0.001)
    
    # B∆∞·ªõc 3: Compile m√¥ h√¨nh
    print("\n" + "=" * 50)
    print("B∆Ø·ªöC 3: COMPILE M√î H√åNH")
    print("=" * 50)
    model.compile_model(learning_rate=1e-3)
    
    # B∆∞·ªõc 4: Hu·∫•n luy·ªán top layers
    print("\n" + "=" * 50)
    print("B∆Ø·ªöC 4: HU·∫§N LUY·ªÜN TOP LAYERS")
    print("=" * 50)
    model.train_top_layers(epochs=20, patience=10)
    
    # B∆∞·ªõc 5: Fine-tuning
    print("\n" + "=" * 50)
    print("B∆Ø·ªöC 5: FINE-TUNING")
    print("=" * 50)
    model.fine_tune_model(unfreeze_layers=20, learning_rate=1e-5, epochs=30)
    
    # B∆∞·ªõc 6: ƒê√°nh gi√° m√¥ h√¨nh
    print("\n" + "=" * 50)
    print("B∆Ø·ªöC 6: ƒê√ÅNH GI√Å M√î H√åNH")
    print("=" * 50)
    loss, accuracy = model.evaluate_model()
    
    # B∆∞·ªõc 7: V·∫Ω bi·ªÉu ƒë·ªì
    print("\n" + "=" * 50)
    print("B∆Ø·ªöC 7: V·∫º BI·ªÇU ƒê·ªí")
    print("=" * 50)
    model.plot_training_history()
    
    # B∆∞·ªõc 8: L∆∞u m√¥ h√¨nh
    print("\n" + "=" * 50)
    print("B∆Ø·ªöC 8: L∆ØU M√î H√åNH")
    print("=" * 50)
    model.save_model('helmet_detection_final.keras')
    
    print("\n" + "=" * 70)
    print("üéâ HO√ÄN TH√ÄNH HU·∫§N LUY·ªÜN!")
    print("=" * 70)
    print(f"üìä Final Validation Accuracy: {accuracy:.4f}")
    print(f"üìä Final Validation Loss: {loss:.4f}")

if __name__ == "__main__":
    main()
